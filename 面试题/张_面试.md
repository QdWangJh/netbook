# `项目问答

## ==每天任务数量\跑批数量==---------一定要问

千万条

## ==shell脚本------------------------一定要问==

- 用的模板
- 关键点问一下(桌面有保存)
- I:\个人

## ==上线流程==

- 项目上线流程
  - 埋点 
  - 生产数据
  - 开发大会
  - 明确数据
  - 数据预处理
  - 数据分析
  - 数据应用
  - 自动化测试
  - 上线
  - 校验
  - 回退
  - 维护迭代

## ==表命名规范==

- 正式表
  - 层次_主题域_主题_业务描述_时间后缀
- 临时表
  - tmp_名字全程_对应的正式表名_自由发挥_yymmdd	
- 主题域
  - 要有缩写	
  - 指标命名规范
  - 命名字段使用小写字母,单词之间下划线分开,长度不超过30字符
- 建表规范
  - 临时表采用内部表
  - 正式表外部表
  - 按照模型层次	
  - 避免使用关键字

## ==元数据管理==

- apache atlas

## 你了解的调度系统有那些？，你们公司用的是哪种调度系统

- airflow，azkaban，ooize，我们公司使用的是oozie

## ==数据太大怎么办==

- 先放在磁盘,将hdfs集群的数据挂载磁盘,退掉磁盘删除hdfs的数据
- ods删除周期
- 其他层永久保存

## 常用函数

- 函数：怎么调用不同的函数来实现不同的处理
- 字符串处理：length/substr/split/regex_replace
- 日期处理：day，year,month,unix_timestamp,from_unixtime
- JSON处理：get_json_object,json_tuple
- URL处理：parse_url_tuple
- 窗口函数：sum/max/min/count、lead/lag/first_value/last_value、row_nubmer/desenrank
- 其他函数：explode/case when /if /concat_ws/collect_list



## 遇到过什么问题

- mysql底层是null \hive是\n导入导出的时候要配置non string的参数和string来保证当Value是NULL，替换成指定的字符
- Sqoop内存溢出,以前是sqoop1.3,没有设置map
  - 因为map客户端会向数据库发送查询语句，将会拿到所有数据到map的客户端缓存到，然后在执行map()方法一条一条处理，所有如果设置不好，一个map拿到的表数据过大就会内存溢出，毕竟里面是用jdbc去获取的，所有数据都装在jdbc的对象中，爆是必然的。在1.3以后改写jdbc的内部原理，拿到一条数据就写入硬盘中，就没有内存溢出了
  - 
- spark设置了executor数量但是还是不起作用
  - 因为没有设置stage默认的task数量\分区数,spark自动分配的task较少,没有和executor对应气来

## **==还有这个数据倾斜是在什么地方出现的，哪些任务出现过数据倾斜==**---一定要解决

- groupby
- count
- distinct
- join
- 员工表和业务工单

## 传感器

- LED(光照联动\电影模式\)
- 报警灯(安防离家模式)
- 排风扇(燃气报警器联动\温湿度联动)
- 窗帘(光照\电影模式\设备自动化)
- 燃气(报警安防)	温湿度()	光照()
- 红外(电视DVD所有可以红外操控的\包括传统家电和兼容其他品牌家电)
- 门禁系统(安防离家模式\远程开门)
- 监控系统(安防离家模式)
- 空调新风(温湿度联动)

- 智能水表电表

## Ubox内kafka之前的数据怎么来的

- netty接收数据,解析header和body
- channel暂存数据
- 解析数据sink发送到kafka中
- 户内机与netty建立tcp连接接收数据
- tcp source将netty channel数据解析 boday的数据体
- 解析后的消息数据为json格式,发送到kafka channel中
- 发送数据到kafka,使用轮询机制发送到多个kafka分区

​	spark消费kafka数据,进行etl![网关数据来源](C:\Users\Zhang\Desktop\网关数据来源.png)



## sqoop怎么导入数据

- append 只导入增加的数据
- lastmodified必须有动态时间变化列
- check-column
- last-value
- createtime updatetime==直接用select==

## sqoop怎么导出

- allowinsert
- updatekey
- updatemode
- 封装脚本和job任务
- 密码文件

## ==全量和增量的区别==

- 两个表主键关联,unionall
- 两个表根据orderid去重,根据order分组
- da表每天的分区就是当天的数据日志表

## ==什么情况下拉链表,用的多吗==--------一定要问

![1636275997356](C:\Users\Zhang\AppData\Roaming\Typora\typora-user-images\1636275997356.png)

- 工单状态
- 仓储状态

## ==日志数据内容==

```
192.168.88.1^A1628344493.393^A192.168.88.130^A/hpsk.jpg?en=e_pv&p_url=http%3A%2F%2Flocalhost%3A8080%2Fhpsk_sdk%2Fdemo2.html&p_ref=http%3A%2F%2Flocalhost%3A8080%2Fhpsk_sdk%2Fdemo.html&tt=%E6%B5%8B%E8%AF%95%E9%A1%B5%E9%9D%A22&ver=1&pl=website&sdk=js&u_ud=863AABBD-DBF5-44F4-B043-C2BFE36F2B95&u_sd=E97B794F-B46B-4DEA-8481-1613187BE268&c_time=1628413284855&l=zh-CN&b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20Win64%3B%20x64)%20AppleWebKit%2F537.36%20(KHTML%2C%20like%20Gecko)%20Chrome%2F92.0.4515.131%20Safari%2F537.36&b_rst=1536*864
```

- 第一个部分：基本请求数据：以^A
  - 192.168.88.1：用户的ip地址，用于构建地理纬度或者统计IP访问
  - 1628344493.393：时间戳，用户访问网站的时间
  - 192.168.88.130：服务端的IP地址
  - URI：所有用户参数
- 第二部分：收集到的用户数据：用&分割
  - en=e_pv：事件类型，pv表示是浏览操作，crt：请求下订单，rg
    - 应用：事件统计
    - 用户产生的不同的行为，收集到的每条数据的内容是一样的
  - p_url=http%3A%2F%2Flocalhost%3A8080%2Fhpsk_sdk%2Fdemo2.html
    - url：表示用户正在访问哪个页面
    - 应用：一般用于访问分析
  - p_ref=http%3A%2F%2Flocalhost%3A8080%2Fhpsk_sdk%2Fdemo.html
    - refere：表示用户访问这个页面是从哪个页面进来的
    - 应用：一般用于来源分析
  - pl=website：用户请求平台，website、h5、app、小程序
  - u_ud=863AABBD-DBF5-44F4-B043-C2BFE36F2B95
    - uuid/uid/guid：访问用户的id
    - 应用：唯一标识一个用户，用于统计UV等等
  - u_sd=E97B794F-B46B-4DEA-8481-1613187BE268
    - sessionId，usid：会话Id，每一个会话Id标识一次访问
  - b_iev=Mozilla%2F5.0%20(Windows%20NT%2010.0%3B%20Win64%3B%20x64)%20AppleWebKit%2F537.36%20(KHTML%2C%20like%20Gecko)%20Chrome%2F92.0.4515.131%20Safari%2F537.36
    - agent：客户端信息
    - 应用：可以获取用户的操作系统版本，浏览器版本，构建用户画像

## ==核心字段==

- 信息收集表字段
  - 主键
  - 访客id
  - 地区
  - 开始时间
  - 结束时间
  - 业务人员
  - 最后一条消息时间
  - 浏览器名称
  - 系统名称
  - 历史记录
  - 关联字段
- 传感器日志
  - 运行状态
  - 温度湿度流明度
  - 连接状态
  - 连接模式
  - 时间
  - mac地址
  - 出场时间

## == 常见指标\维度 ==

- 维度表
  - 日期维度
  - 地区维度
  - 服务网点维度
  - 仓库维度
  - 工单维度
  - 信息维度
  - 故障维度
- 事实表
  - 呼叫中心事实表
  - 工单事实表
  - 网点事实表
  - 安装事实表
  - 售后事实表
  - 回访事实表
  - 费用事实表
  - 差旅事实表
  - 仓库事实表

- 指标

  - 服务指标
  - 客户类型指标
  - 仓库指标

- 流量分析模块

  - pv 

  - uv

  - ip 

  - 跳出 

  - 二跳

  - 来源

  - 全站流量

  - 日活

  - 注册

    

- 受访分析模块 

  - 受访topn
  - 热点topn
  - 平均访问数量
  - 平均访问时长
  - 下单
  - 转化率

  

## ==处理逻辑==

- 过滤
  - 不需要的行和列
  - 列的过滤 select后面不进行查询
  - 行的过滤 deleted=0
  - 特殊字段 

- 补全
  
- 基于年月日小时分秒
  
- 转换

  - 电话报名和线上报名用0和1标记

    

- 信息主题
  -  每小时的访问转咨询率
  - 每天总访问客户数量
  - 各个来源渠道用户占比
- 原始数据etl

## ==聚合过哪些表，聚合过哪些指标==

- 时间维度和售后事实表
- 地区维度和设备故障事实表
- 

## ==大概问了哪几层，为什么这么分==

- 五层 
  - ods原数据层
  - dwd数据明细层
  - dwm数据中间层
  - dws数据汇总层
  - app数据应用层

- ods与业务处数据基本一致
- dwd对数据规范化,比如时间,编码转换,二进制,脱敏,清洗
- dwm基于主题关联事实表和维度表
- dws汇总通用指标和维度,和时间拉宽
- app应用层展示

## ==表命名缩写==

- 全量表df
- 增量表di
- 追加表da
- 拉链表dz

## ==数据规划数据膨胀的处理==-----一定要问

- 根据表数据和后续业务发展抽样方案服务器压力测试

## ==数仓app层怎么对外提供服务==

- 直接接入mysql业务库,业务方直接读取
- 接口形式读取
- jdbc

## ==mysql权限==

- 新建交付表,给权限

## ==avro格式==

- avro 特殊的二进制文件,保存schema信息
- oracle数据导出\r\n



## ==服务器配置==主要看数仓准备-----------一定要解决

通用配置 cpu和内存 1:2   

计算节点cup和内存1:1

存储管理内存的节点 cpu和内存1:4



- 传感器数据大小 
  - 一条数据1k
  - 3-5秒一条
  - 一个设备一天就是17280条数据,一天就是17280KB=16MB
  - 
  - 
  - 
  - 设备数量5120台
  - 每天的数据量80GB



- 网页类数据

  - UV---------100W访问

  - 平均每个人访问五次

  - 每次访问五个页面

  - 天数据增量-----5puX5次X100万X2kb/1024k/1024m=47GB

  - 每天数据增量---2500W条

    

- 数据保存

  - 每天数据总量50GB+80GB=130GB
- 5年----237250gb=230tb
  
  
  
  
  
- 230tb/0.8=289tb
  - 300tb 10tb一台=30台dn









- ==机器数==
- 需要的dn数量 30台 
  - zk3台 
  - nnha 2台
  - jn 3台
  - 集群一共需要 23台





核心数

128GB



- ==数据变化==

  每天47GB(uvpv)

  ods 47GB=>10G

  dwd 10G

  dws+ads 50G 





- kafka里面的数据

  - 100G 2副本 3天 0.7 =
- javaee等后台数据量
- 总数据量
- 一台服务器 20核40线程128GB内存 